{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bayesian Recovery Obsidian, Zotero and Git Technical Set Up","title":"Home"},{"location":"Bayesian%20Recovery/","tags":["blog","knownTheory","longform"],"text":"Bayesian Recovery \u00b6 Introduction \u00b6 This blog post is an attempt at a an alternative presentation of the results and ideas presented in this paper . Body \u00b6 We first set out to describe the problem and what we seek as a result. Given a signal \\(z^* \\in \\mathbb{R}^n\\) distributed according to some (prior) distribution \\(P_{z}\\) we measure \\(z^*\\) with a random gaussian measurement matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) with \\(m \\leq n\\) where each entry \\(A_{ij}\\) is distributed i.i.d. \\(\\sim \\mathcal{N}\\left( 0, \\frac{1}{m} \\right)\\) which gives us the (then known) measurement vector \\(y:= Az^*\\) . Once \\(y\\) is known, we examine the posterior variable \\(\\hat{z} \\sim p_{z^*|y}(\\cdot)\\) . Formally, we define \\(\\hat{z}\\) as the unique random variable satisfying the following two properties: \\(\\hat{z}\\) is distributed according to the same distribution as \\(z^*\\) \\(\\hat{z}\\) is conditionally independent of \\(z^*\\) with respect to \\(y\\) ; this means that the dependence between the random variables \\(\\hat{z}\\) and \\(z^*\\) only lies in the fact that \\(Az^*=y=A\\hat{z}\\) . Notice that these two properties are sufficiently constraining to make \\(\\hat{z}\\) uniquely defined. We will try to establish that with high probability, \\(\\hat{z}\\) is no more than \\(\\varepsilon\\) -away from \\(z^*\\) . This last sentence is what we call \"signal recovery\" because we can understand \\(\\hat{z}\\) as the conditioned \\(z^*\\) once \\(y\\) is known. Hence \\(\\hat{z}\\) is the posterior random variable To show the result, we partition the support of \\(z^*\\) into small components \\(\\{ E_{i} \\}\\) with each \\(E_{i} \\subseteq \\mathbb{R}^n\\) of radius no more than \\(\\frac{\\varepsilon}{2}\\) , and show that with high probability \\(z^*\\) and \\(\\hat{z}\\) lie in the same component. Part 1: Splitting the Prior Into Components \u00b6 We will use dependence on some deterministic random variable to split the prior distribution into sub-distributions that are each supported on a single component of the support. Introduce the random variable \\(c^* \\in \\{ 0,\\dots,M \\}\\) which we make dependent on \\(z^*\\) in a way that lets us make a good split \\[ P_{z^*} = \\sum_{i=1}^M P_{z^*}(\\cdot|c^*=i)P(c=i). \\] We make a smart split so that each distribution \\(P_{z^*}(\\cdot|c^*=i)\\) is supported in a single component \\(E_{i}\\) of radius no more than \\(\\frac{\\varepsilon}{2}\\) . In the event that \\(\\hat{z}\\) lands in the same component as \\(z^*\\) , we get approximate recovery. We will show that this happens with high probability. We define \\(\\hat{c}\\) such that it is dependent on \\(\\hat{z}\\) in exactly the same way as \\(c^*\\) is dependent on \\(z^*\\) . Then the event of failure is \\(\\{\\hat{c} \\neq c^*\\}\\) . Our aim is now to show that this event occurs with low probability, specifically exponentially small in the number of measurements. Part 2: Bounding the Probability of Failure \u00b6 We wish to bound \\(P(\\hat{c} \\neq c^*)\\) . First we decompose this term by values of \\(c^*\\) . \\[ P(\\hat{c} \\neq c^*) = \\sum_{i = 1}^M P(\\hat{c} \\neq i, c^* =i) \\] For simplicity, we will focus on evaluating \\(P(\\hat{c} \\neq 0, c^* = 0)\\) . We will proceed by letting the event \\(\\{ c^* = 0 \\}\\) occur first, and then decompose on all the possibilities of y. \\( \\(P(\\hat{c}\\neq 0, c^*=0) = P(c^*=0)P(\\hat{c} \\neq 0|c^*=0) = P(c^* =0) \\int P(\\hat{c} \\neq 0|y, c^*=0)p_{y}(y| c^*=0) \\, dy\\) \\) Then we use the fact that \\(P(\\hat{c} \\neq 0 | y, c^*=0) = P(\\hat{c} \\neq 0|y)\\) as a direct application of conditional independence. \\[\\dots = P(c^* =0) \\int P(\\hat{c} \\neq 0|y)p_{y}(y|c^*=0) \\, dy.\\] Now is the time to apply Bayes rule. We do so on the first factor. Because of the dependence of \\(y\\) on the integral, this converts to a density function in \\(y\\) . \\[ \\dots=P(c^*=0)\\int p(y|\\hat{c}\\neq 0) \\frac{P(\\hat{c} \\neq 0)}{p(y)} p(y|c^*=0) \\, dy. \\] Now we can perform the expansion \\(p(y) = P(\\hat{c}=0)p(y|\\hat{c}=0) + P(\\hat{c} \\neq 0)p(y|\\hat{c}\\neq 0)\\) . Plugging this into the above, and separating by the denominator, \\[\\dots \\leq \\int \\frac{P(c^*=0) p(y|c^*=0)P(\\hat{c}\\neq 0)p(y|\\hat{c} \\neq 0)}{2\\max[P(\\hat{c} = 0)p(y|\\hat{c}=0), P(\\hat{c} \\neq 0)p(y|\\hat{c} \\neq 0)]} \\, dy\\] Now notice that by the fact that \\(c^*\\) and \\(\\hat{c}\\) are identically distributed, \\(P(c^*=0) = P(\\hat{c}=0)\\) and \\(p(y|c^*=0) = p(y|\\hat{c}=0)\\) . Plugging this in, \\[ \\dots = \\int \\min\\{P(\\hat{c}=0)p(y|\\hat{c}=0), P(\\hat{c}\\neq 0)p(y|\\hat{c} \\neq 0)\\} \\, dy \\] \\[ \\leq \\int \\min\\{p(y|\\hat{c}=0),p(y|\\hat{c}\\neq 0)\\} \\, dy \\] \\[ =1 - TV(p_{y}(\\cdot|\\hat{c}=0), p_{y}(\\cdot|\\hat{c} \\neq 0)) \\] the total variation term then corresponds to the success probability. In one phrase: the success probability is at least the total variation of the conditional distributions of the measurement vector. Part 3: Bringing it Together \u00b6 \\[ P(\\hat{c} \\neq c^*) = \\sum_{i=1}^M P(c^*=i, \\hat{c} \\neq i) \\] \\[ \\leq M \\max_{i}(1-TV(p_{y}(\\cdot|\\hat{c}=i), p_{y}(\\cdot|\\hat{c} \\neq i))). \\] This factor of \\(M\\) can probably be removed because of the loose upper bound from before. Notice that when the total variation term is large, the probability of failure goes to zero. Hence we find that we have recovery with high probability. Main source: [[@jalalInstanceOptimalCompressedSensing2021]] [[2022-11-01]]","title":"Bayesian Recovery"},{"location":"Bayesian%20Recovery/#bayesian-recovery","text":"","title":"Bayesian Recovery"},{"location":"Bayesian%20Recovery/#introduction","text":"This blog post is an attempt at a an alternative presentation of the results and ideas presented in this paper .","title":"Introduction"},{"location":"Bayesian%20Recovery/#body","text":"We first set out to describe the problem and what we seek as a result. Given a signal \\(z^* \\in \\mathbb{R}^n\\) distributed according to some (prior) distribution \\(P_{z}\\) we measure \\(z^*\\) with a random gaussian measurement matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) with \\(m \\leq n\\) where each entry \\(A_{ij}\\) is distributed i.i.d. \\(\\sim \\mathcal{N}\\left( 0, \\frac{1}{m} \\right)\\) which gives us the (then known) measurement vector \\(y:= Az^*\\) . Once \\(y\\) is known, we examine the posterior variable \\(\\hat{z} \\sim p_{z^*|y}(\\cdot)\\) . Formally, we define \\(\\hat{z}\\) as the unique random variable satisfying the following two properties: \\(\\hat{z}\\) is distributed according to the same distribution as \\(z^*\\) \\(\\hat{z}\\) is conditionally independent of \\(z^*\\) with respect to \\(y\\) ; this means that the dependence between the random variables \\(\\hat{z}\\) and \\(z^*\\) only lies in the fact that \\(Az^*=y=A\\hat{z}\\) . Notice that these two properties are sufficiently constraining to make \\(\\hat{z}\\) uniquely defined. We will try to establish that with high probability, \\(\\hat{z}\\) is no more than \\(\\varepsilon\\) -away from \\(z^*\\) . This last sentence is what we call \"signal recovery\" because we can understand \\(\\hat{z}\\) as the conditioned \\(z^*\\) once \\(y\\) is known. Hence \\(\\hat{z}\\) is the posterior random variable To show the result, we partition the support of \\(z^*\\) into small components \\(\\{ E_{i} \\}\\) with each \\(E_{i} \\subseteq \\mathbb{R}^n\\) of radius no more than \\(\\frac{\\varepsilon}{2}\\) , and show that with high probability \\(z^*\\) and \\(\\hat{z}\\) lie in the same component.","title":"Body"},{"location":"Bayesian%20Recovery/#part-1-splitting-the-prior-into-components","text":"We will use dependence on some deterministic random variable to split the prior distribution into sub-distributions that are each supported on a single component of the support. Introduce the random variable \\(c^* \\in \\{ 0,\\dots,M \\}\\) which we make dependent on \\(z^*\\) in a way that lets us make a good split \\[ P_{z^*} = \\sum_{i=1}^M P_{z^*}(\\cdot|c^*=i)P(c=i). \\] We make a smart split so that each distribution \\(P_{z^*}(\\cdot|c^*=i)\\) is supported in a single component \\(E_{i}\\) of radius no more than \\(\\frac{\\varepsilon}{2}\\) . In the event that \\(\\hat{z}\\) lands in the same component as \\(z^*\\) , we get approximate recovery. We will show that this happens with high probability. We define \\(\\hat{c}\\) such that it is dependent on \\(\\hat{z}\\) in exactly the same way as \\(c^*\\) is dependent on \\(z^*\\) . Then the event of failure is \\(\\{\\hat{c} \\neq c^*\\}\\) . Our aim is now to show that this event occurs with low probability, specifically exponentially small in the number of measurements.","title":"Part 1: Splitting the Prior Into Components"},{"location":"Bayesian%20Recovery/#part-2-bounding-the-probability-of-failure","text":"We wish to bound \\(P(\\hat{c} \\neq c^*)\\) . First we decompose this term by values of \\(c^*\\) . \\[ P(\\hat{c} \\neq c^*) = \\sum_{i = 1}^M P(\\hat{c} \\neq i, c^* =i) \\] For simplicity, we will focus on evaluating \\(P(\\hat{c} \\neq 0, c^* = 0)\\) . We will proceed by letting the event \\(\\{ c^* = 0 \\}\\) occur first, and then decompose on all the possibilities of y. \\( \\(P(\\hat{c}\\neq 0, c^*=0) = P(c^*=0)P(\\hat{c} \\neq 0|c^*=0) = P(c^* =0) \\int P(\\hat{c} \\neq 0|y, c^*=0)p_{y}(y| c^*=0) \\, dy\\) \\) Then we use the fact that \\(P(\\hat{c} \\neq 0 | y, c^*=0) = P(\\hat{c} \\neq 0|y)\\) as a direct application of conditional independence. \\[\\dots = P(c^* =0) \\int P(\\hat{c} \\neq 0|y)p_{y}(y|c^*=0) \\, dy.\\] Now is the time to apply Bayes rule. We do so on the first factor. Because of the dependence of \\(y\\) on the integral, this converts to a density function in \\(y\\) . \\[ \\dots=P(c^*=0)\\int p(y|\\hat{c}\\neq 0) \\frac{P(\\hat{c} \\neq 0)}{p(y)} p(y|c^*=0) \\, dy. \\] Now we can perform the expansion \\(p(y) = P(\\hat{c}=0)p(y|\\hat{c}=0) + P(\\hat{c} \\neq 0)p(y|\\hat{c}\\neq 0)\\) . Plugging this into the above, and separating by the denominator, \\[\\dots \\leq \\int \\frac{P(c^*=0) p(y|c^*=0)P(\\hat{c}\\neq 0)p(y|\\hat{c} \\neq 0)}{2\\max[P(\\hat{c} = 0)p(y|\\hat{c}=0), P(\\hat{c} \\neq 0)p(y|\\hat{c} \\neq 0)]} \\, dy\\] Now notice that by the fact that \\(c^*\\) and \\(\\hat{c}\\) are identically distributed, \\(P(c^*=0) = P(\\hat{c}=0)\\) and \\(p(y|c^*=0) = p(y|\\hat{c}=0)\\) . Plugging this in, \\[ \\dots = \\int \\min\\{P(\\hat{c}=0)p(y|\\hat{c}=0), P(\\hat{c}\\neq 0)p(y|\\hat{c} \\neq 0)\\} \\, dy \\] \\[ \\leq \\int \\min\\{p(y|\\hat{c}=0),p(y|\\hat{c}\\neq 0)\\} \\, dy \\] \\[ =1 - TV(p_{y}(\\cdot|\\hat{c}=0), p_{y}(\\cdot|\\hat{c} \\neq 0)) \\] the total variation term then corresponds to the success probability. In one phrase: the success probability is at least the total variation of the conditional distributions of the measurement vector.","title":"Part 2: Bounding the Probability of Failure"},{"location":"Bayesian%20Recovery/#part-3-bringing-it-together","text":"\\[ P(\\hat{c} \\neq c^*) = \\sum_{i=1}^M P(c^*=i, \\hat{c} \\neq i) \\] \\[ \\leq M \\max_{i}(1-TV(p_{y}(\\cdot|\\hat{c}=i), p_{y}(\\cdot|\\hat{c} \\neq i))). \\] This factor of \\(M\\) can probably be removed because of the loose upper bound from before. Notice that when the total variation term is large, the probability of failure goes to zero. Hence we find that we have recovery with high probability. Main source: [[@jalalInstanceOptimalCompressedSensing2021]] [[2022-11-01]]","title":"Part 3: Bringing it Together"},{"location":"Obsidian%2C%20Zotero%20and%20Git%20Technical%20Set%20Up/","text":"blog \u00b6 Technical set-up: Obsidian Download Obsidian Create Personal Vault Folder structure Files References Notes (or Zettelkasten) Templates Quick switcher Command Pane Tags vs Links add Dataview Home note -> get Add Calendar Plugin (optional) Git Download git (desktop) and create github account if necessary Add collaborators to repository. Clone repository Zotero Install Zotero https://www.zotero.org/download/ Install BetterBibtex to zotero https://retorque.re/zotero-better-bibtex/installation/ Install Zotero Connector in your browser. Install Zotero Integration Plugin Configure Settings: Add pre-made template to Templates folder add it to Template file -> get Add import format Output path: References/@{{citekey}}.md Note import location: References/ Optional: Overleaf Zotero integration Guiding Philosophy: Zettelkasten \u00b6 [[Ideas of Zettlekasten Slides]]","title":"Obsidian, Zotero and Git Technical Set Up"},{"location":"Obsidian%2C%20Zotero%20and%20Git%20Technical%20Set%20Up/#blog","text":"Technical set-up: Obsidian Download Obsidian Create Personal Vault Folder structure Files References Notes (or Zettelkasten) Templates Quick switcher Command Pane Tags vs Links add Dataview Home note -> get Add Calendar Plugin (optional) Git Download git (desktop) and create github account if necessary Add collaborators to repository. Clone repository Zotero Install Zotero https://www.zotero.org/download/ Install BetterBibtex to zotero https://retorque.re/zotero-better-bibtex/installation/ Install Zotero Connector in your browser. Install Zotero Integration Plugin Configure Settings: Add pre-made template to Templates folder add it to Template file -> get Add import format Output path: References/@{{citekey}}.md Note import location: References/ Optional: Overleaf Zotero integration","title":"blog"},{"location":"Obsidian%2C%20Zotero%20and%20Git%20Technical%20Set%20Up/#guiding-philosophy-zettelkasten","text":"[[Ideas of Zettlekasten Slides]]","title":"Guiding Philosophy: Zettelkasten"}]}